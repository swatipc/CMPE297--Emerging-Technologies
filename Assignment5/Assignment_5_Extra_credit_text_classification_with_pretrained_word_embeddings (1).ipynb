{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_5_Extra_credit_text_classification_with_pretrained_word_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIJAljzF9B6R"
      },
      "source": [
        "#Text classification with pretrained word embeddings (GLOve) \r\n",
        "\r\n",
        "This example trains a text classification model using pre-trained word embeddings. (GloVe embeddings)\r\n",
        "\r\n",
        "I have used Newsgroup20 dataset which consists of a set of 20,000 message board messages belonging to 20 different topic categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLMD4Re2wyf"
      },
      "source": [
        "# Import required libraries\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import pathlib\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtsx3W0_2wyg"
      },
      "source": [
        "## Downloading the Newsgroup20 dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6nmv7fM2wyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0814dc3d-26f4-4dfb-8cd8-ee6774cb0490"
      },
      "source": [
        "dataset_path = keras.utils.get_file(\n",
        "    \"news20.tar.gz\",\n",
        "    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n",
        "    untar=True,\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\n",
            "17334272/17329808 [==============================] - 9s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6ZkjsH92wyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "044856cb-6270-4d10-d374-1c818860b83c"
      },
      "source": [
        "dataset_dir = pathlib.Path(dataset_path).parent / \"20_newsgroup\"\n",
        "dir_names = os.listdir(dataset_dir)\n",
        "print(\"Number of directories: {} \".format(len(dir_names)))\n",
        "print(\"Directory names: {}\".format(dir_names))\n",
        "\n",
        "f_names = os.listdir(dataset_dir / \"comp.graphics\")\n",
        "print(\"Number of files in comp.graphics: {}\".format(len(f_names)))\n",
        "print(\"Some example filenames: {}\".format(f_names[:5]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of directories: 20 \n",
            "Directory names: ['rec.autos', 'comp.sys.mac.hardware', 'rec.sport.baseball', 'talk.politics.guns', 'rec.sport.hockey', 'comp.sys.ibm.pc.hardware', 'sci.med', 'comp.windows.x', 'rec.motorcycles', 'comp.graphics', 'comp.os.ms-windows.misc', 'talk.religion.misc', 'sci.electronics', 'soc.religion.christian', 'talk.politics.misc', 'alt.atheism', 'sci.crypt', 'misc.forsale', 'sci.space', 'talk.politics.mideast']\n",
            "Number of files in comp.graphics: 1000\n",
            "Some example filenames: ['38239', '37962', '39028', '38713', '38262']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIdOcMPL2wyh"
      },
      "source": [
        "Checking the file content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd18Tme62wyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9dccb6d-5473-44c1-9d95-41d108ce336a"
      },
      "source": [
        "print(open(dataset_dir / \"comp.graphics\" / \"38987\").read())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Newsgroups: comp.graphics\n",
            "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!noc.near.net!howland.reston.ans.net!agate!dog.ee.lbl.gov!network.ucsd.edu!usc!rpi!nason110.its.rpi.edu!mabusj\n",
            "From: mabusj@nason110.its.rpi.edu (Jasen M. Mabus)\n",
            "Subject: Looking for Brain in CAD\n",
            "Message-ID: <c285m+p@rpi.edu>\n",
            "Nntp-Posting-Host: nason110.its.rpi.edu\n",
            "Reply-To: mabusj@rpi.edu\n",
            "Organization: Rensselaer Polytechnic Institute, Troy, NY.\n",
            "Date: Thu, 29 Apr 1993 23:27:20 GMT\n",
            "Lines: 7\n",
            "\n",
            "Jasen Mabus\n",
            "RPI student\n",
            "\n",
            "\tI am looking for a hman brain in any CAD (.dxf,.cad,.iges,.cgm,etc.) or picture (.gif,.jpg,.ras,etc.) format for an animation demonstration. If any has or knows of a location please reply by e-mail to mabusj@rpi.edu.\n",
            "\n",
            "Thank you in advance,\n",
            "Jasen Mabus  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmbCtimu2wyh"
      },
      "source": [
        "\n",
        "As the header lines are revealing the file's category, we will delete header section from all files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwY0ViNw2wyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35d59d8f-387d-47e8-8b38-f0c7241ed92d"
      },
      "source": [
        "sample = []\n",
        "label = []\n",
        "class_name = []\n",
        "class_index = 0\n",
        "for dir_name in sorted(os.listdir(dataset_dir)):\n",
        "    class_name.append(dir_name)\n",
        "    dir_path = dataset_dir / dir_name\n",
        "    f_names = os.listdir(dir_path)\n",
        "    print(\"Processing %s, %d files found\" % (dir_name, len(f_names)))\n",
        "    for f in f_names:\n",
        "        fpath = dir_path / f\n",
        "        f = open(fpath, encoding=\"latin-1\")\n",
        "        content = f.read()\n",
        "        lines = content.split(\"\\n\")\n",
        "        lines = lines[10:]\n",
        "        content = \"\\n\".join(lines)\n",
        "        sample.append(content)\n",
        "        label.append(class_index)\n",
        "    class_index += 1\n",
        "\n",
        "print(\"Classes: {}\".format(class_name))\n",
        "print(\"Number of sample: {}\".format(len(sample)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing alt.atheism, 1000 files found\n",
            "Processing comp.graphics, 1000 files found\n",
            "Processing comp.os.ms-windows.misc, 1000 files found\n",
            "Processing comp.sys.ibm.pc.hardware, 1000 files found\n",
            "Processing comp.sys.mac.hardware, 1000 files found\n",
            "Processing comp.windows.x, 1000 files found\n",
            "Processing misc.forsale, 1000 files found\n",
            "Processing rec.autos, 1000 files found\n",
            "Processing rec.motorcycles, 1000 files found\n",
            "Processing rec.sport.baseball, 1000 files found\n",
            "Processing rec.sport.hockey, 1000 files found\n",
            "Processing sci.crypt, 1000 files found\n",
            "Processing sci.electronics, 1000 files found\n",
            "Processing sci.med, 1000 files found\n",
            "Processing sci.space, 1000 files found\n",
            "Processing soc.religion.christian, 997 files found\n",
            "Processing talk.politics.guns, 1000 files found\n",
            "Processing talk.politics.mideast, 1000 files found\n",
            "Processing talk.politics.misc, 1000 files found\n",
            "Processing talk.religion.misc, 1000 files found\n",
            "Classes: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
            "Number of sample: 19997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLWqILP42wyi"
      },
      "source": [
        "## Shuffle and split the data into training and validation datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqDvwCwc2wyi"
      },
      "source": [
        "# Shuffling the data\n",
        "seed = 1337\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(sample)\n",
        "rng = np.random.RandomState(seed)\n",
        "rng.shuffle(label)\n",
        "\n",
        "# Extracting a training & validation split\n",
        "validation_split = 0.2\n",
        "num_validation_sample = int(validation_split * len(sample))\n",
        "train_sample = sample[:-num_validation_sample]\n",
        "val_sample = sample[-num_validation_sample:]\n",
        "train_label = label[:-num_validation_sample]\n",
        "val_label = label[-num_validation_sample:]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEXEMeCc2wyi"
      },
      "source": [
        "## Create a vocabulary index\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEqkJC572wyj"
      },
      "source": [
        "# Using TextVectorization to index vocabulary present in dataset.\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=100)  \n",
        "text_dataset = tf.data.Dataset.from_tensor_slices(train_sample).batch(128)\n",
        "vectorizer.adapt(text_dataset)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_B5WKQ92wyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f8c6906-867a-4927-8bb9-121748bc4bf1"
      },
      "source": [
        "# Printing top 5 words from the computed vocabulary\r\n",
        "vectorizer.get_vocabulary()[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', 'the', 'to', 'of']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8hjxB_42wyj"
      },
      "source": [
        "Let's vectorize a test sentence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuTKVLFL2wyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d9b4486-090c-4936-85c4-43ff6effc897"
      },
      "source": [
        "# Vectorizing the sample sentence\n",
        "output = vectorizer([[\"the cat sat on the yellow mat\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   2, 3811, 1713,   15,    2, 5115])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEHysIkP2wyk"
      },
      "source": [
        "# Mapping words to indices\n",
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm1cNIZA2wyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb3a9ca-ad3a-49d5-e8ed-32c42f7bb4be"
      },
      "source": [
        "test_sen = [\"the\", \"cat\", \"sat\", \"on\", \"the\",'yellow', \"mat\"]\n",
        "[word_index[w] for w in test_sen]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3811, 1713, 15, 2, 5115, 6091]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fwsk5oHZ1B7"
      },
      "source": [
        "The obtained encoding is same as above encoding of our sample sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "essnbj7q2wyk"
      },
      "source": [
        "## Load pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCaNGiVpaQKr",
        "outputId": "6acfc91e-0c6f-46f7-efcf-6dbab3795921"
      },
      "source": [
        "# Download pre-trained GloVe embeddings\r\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\r\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-11 02:25:52--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-12-11 02:25:52--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-12-11 02:25:52--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.94MB/s    in 6m 30s  \n",
            "\n",
            "2020-12-11 02:32:22 (2.11 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaKa13R8afiW",
        "outputId": "f0c33f7d-b1f9-4fec-9c9b-8e43efe94086"
      },
      "source": [
        "# Checking current directory\r\n",
        "!pwd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGcIkWI5amZP",
        "outputId": "d305235f-8e47-4ddb-d346-5f3c32f17dc8"
      },
      "source": [
        "# Checking the files present in current directory\r\n",
        "!ls"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n",
            "glove.6B.200d.txt  glove.6B.50d.txt   sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "044IC9CGddVz"
      },
      "source": [
        "# Making a dict mapping strings to a NumPy vector reproesentation\r\n",
        "path_to_glove = os.path.join(\"/content/glove.6B.100d.txt\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_HVo9b2wyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b8c85d-9dd8-4b36-b307-f14847cbe96d"
      },
      "source": [
        "embedding_index= {}\n",
        "with open(path_to_glove) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "print(\"Found {} word vectors.\".format(len(embedding_index)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1ZSE5Dn2wyl"
      },
      "source": [
        "Prepare a corresponding embedding matrix which can be used in a Keras\n",
        "`Embedding` layer. It's a simple NumPy matrix having entry at index `i` as the pre-trained\n",
        "vector for the word of index `i` in the `vectorizer`'s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07xiOpFr2wyl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f80d65-0e75-4ba5-f32a-32cddef291dd"
      },
      "source": [
        "number_of_tokens = len(voc) + 2\n",
        "embedding_dim = 100\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Preparing a embedding matrix\n",
        "embedding_matrix = np.zeros((number_of_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted {} words ({} misses)\".format(hits, misses))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converted 17984 words (2016 misses)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFf3aryZ2wym"
      },
      "source": [
        "#Loading the pre-trained word embeddings matrix into `Embedding` layer.\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    number_of_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQXVLyj52wym"
      },
      "source": [
        "## Build the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6QmCuIH2wyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252d416e-27e5-47d9-8273-aaede458d1b1"
      },
      "source": [
        "int_sequence_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequence = embedding_layer(int_sequence_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequence)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "pred = layers.Dense(len(class_name), activation=\"softmax\")(x)\n",
        "model_1 = keras.Model(int_sequence_input, pred)\n",
        "model_1.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         2000200   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 128)         64128     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, None, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 20)                2580      \n",
            "=================================================================\n",
            "Total params: 2,247,516\n",
            "Trainable params: 247,316\n",
            "Non-trainable params: 2,000,200\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zNubbSQ2wyn"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "Convert the list-of-strings data to NumPy arrays of integer indices. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX2RVeij2wyn"
      },
      "source": [
        "x_train = vectorizer(np.array([[s] for s in train_sample])).numpy()\n",
        "x_val = vectorizer(np.array([[s] for s in val_sample])).numpy()\n",
        "\n",
        "y_train = np.array(train_label)\n",
        "y_val = np.array(val_label)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sefAQse2wyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1d2409-4d1d-43ae-ce94-f9a4132b848f"
      },
      "source": [
        "model_1.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"])\n",
        "model_1.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "125/125 [==============================] - 5s 36ms/step - loss: 2.6771 - acc: 0.1383 - val_loss: 2.0733 - val_acc: 0.3076\n",
            "Epoch 2/20\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 1.9039 - acc: 0.3450 - val_loss: 1.6290 - val_acc: 0.4261\n",
            "Epoch 3/20\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 1.5062 - acc: 0.4834 - val_loss: 1.2502 - val_acc: 0.5711\n",
            "Epoch 4/20\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 1.2532 - acc: 0.5695 - val_loss: 1.1553 - val_acc: 0.6017\n",
            "Epoch 5/20\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 1.0911 - acc: 0.6269 - val_loss: 1.1219 - val_acc: 0.6177\n",
            "Epoch 6/20\n",
            "125/125 [==============================] - 4s 34ms/step - loss: 0.9626 - acc: 0.6685 - val_loss: 1.1420 - val_acc: 0.6164\n",
            "Epoch 7/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.8480 - acc: 0.7066 - val_loss: 1.0092 - val_acc: 0.6582\n",
            "Epoch 8/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.7438 - acc: 0.7397 - val_loss: 1.0429 - val_acc: 0.6582\n",
            "Epoch 9/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.6621 - acc: 0.7668 - val_loss: 0.9819 - val_acc: 0.6937\n",
            "Epoch 10/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.5646 - acc: 0.7985 - val_loss: 0.9794 - val_acc: 0.6999\n",
            "Epoch 11/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.5077 - acc: 0.8189 - val_loss: 1.1223 - val_acc: 0.6717\n",
            "Epoch 12/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.4484 - acc: 0.8424 - val_loss: 1.4261 - val_acc: 0.6552\n",
            "Epoch 13/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.3905 - acc: 0.8624 - val_loss: 1.1172 - val_acc: 0.7009\n",
            "Epoch 14/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.3372 - acc: 0.8795 - val_loss: 1.1657 - val_acc: 0.6897\n",
            "Epoch 15/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.2975 - acc: 0.8966 - val_loss: 1.1262 - val_acc: 0.7147\n",
            "Epoch 16/20\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.2570 - acc: 0.9116 - val_loss: 1.2137 - val_acc: 0.6924\n",
            "Epoch 17/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.2481 - acc: 0.9157 - val_loss: 1.2019 - val_acc: 0.7007\n",
            "Epoch 18/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.2102 - acc: 0.9301 - val_loss: 1.3157 - val_acc: 0.7042\n",
            "Epoch 19/20\n",
            "125/125 [==============================] - 4s 36ms/step - loss: 0.1975 - acc: 0.9333 - val_loss: 1.4896 - val_acc: 0.7034\n",
            "Epoch 20/20\n",
            "125/125 [==============================] - 4s 35ms/step - loss: 0.1933 - acc: 0.9362 - val_loss: 1.4118 - val_acc: 0.7159\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f9521d710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I6i5peC2wyo"
      },
      "source": [
        "## Export an end-to-end model\n",
        "\n",
        "Export a model to make it portable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSzms07k2wyo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9c8e3fc0-6bcc-45c0-b98b-b98ba9681b91"
      },
      "source": [
        "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
        "x = vectorizer(string_input)\n",
        "preds = model_1(x)\n",
        "end_to_end_model = keras.Model(string_input, preds)\n",
        "\n",
        "probability = end_to_end_model.predict([[\"This message is about computer graphics and 3D modeling\"]])\n",
        "\n",
        "class_name[np.argmax(probability[0])]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'comp.graphics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}